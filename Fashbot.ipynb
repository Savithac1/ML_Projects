{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
{
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "# Fashbot\n",
    "\n",
    "## Abstract\n",
    "Fashbot is an AI-driven fashion recommendation system that analyzes user preferences and trends to generate personalized outfit suggestions. By leveraging machine learning and natural language processing (NLP), the model curates style recommendations based on factors such as season, occasion, and current fashion trends.\n",
    "\n",
    "### **Objective**\n",
    "To develop an intelligent fashion assistant that provides tailored outfit recommendations, helping users navigate fashion choices effortlessly.\n",
    "\n",
    "### **Problem Statement**\n",
    "Choosing the right outfit can be time-consuming and overwhelming due to the vast array of fashion choices available. Traditional recommendation systems often lack personalization and fail to adapt to individual style preferences. Fashbot addresses this gap by using machine learning to deliver dynamic, context-aware fashion recommendations.\n",
    "\n",
    "### **Key Features**\n",
    "- AI-powered fashion recommendation system  \n",
    "- Personalized outfit suggestions based on user preferences  \n",
    "- Trend-aware recommendations using NLP and data retrieval  \n",
    "- Scalable and adaptable to different fashion categories  \n",
    "\n",
    "This implementation can be extended by integrating e-commerce platforms, user feedback loops, and real-time fashion trend analysis to enhance recommendation accuracy.\n"
  ]
},

    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D7LnQ_Wap-2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af9c528-a489-4af3-d64e-342801c44bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.29.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, update_checker, pydeck, prawcore, praw, streamlit\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 pydeck-0.9.1 streamlit-1.43.2 update_checker-0.18.0 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ],
      "source": [
        "# Installing required libraries:\n",
        "# - 'streamlit' for building the web interface of the chatbot\n",
        "# - 'praw' for accessing and interacting with Reddit's API to gather fashion-related data\n",
        "# - 'nltk' for natural language processing tasks (e.g., text preprocessing and tokenization)\n",
        "# - 'pyngrok' for creating a secure tunnel to expose the app on the web (useful for deployment)\n",
        "\n",
        "!pip install streamlit praw nltk\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\"\"\"\n",
        "Fashion Trend Chatbot\n",
        "---------------------\n",
        "This app does the following:\n",
        "1. Connects to Reddit using PRAW.\n",
        "2. Fetches posts from one or more fashion-related subreddits that mention a given keyword.\n",
        "3. Analyzes how many posts mention the keyword in the last day, week, and month.\n",
        "4. Extracts context snippets (5 words before and after) for each occurrence.\n",
        "5. Reports a trend status message based on thresholds.\n",
        "6. Provides an additional function to list overall trending keywords (that appear at least 15 times) in the past 2 days.\n",
        "7. Has a chat section that uses a GPT-2 LLM for general conversation.\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import praw\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import pipeline\n",
        "\n",
        "# --------------------------\n",
        "# 1. Download NLTK Resources\n",
        "# --------------------------\n",
        "# Downloading necessary NLTK datasets for tokenization and stopword removal.\n",
        "nltk.download('punkt')  # For word tokenization\n",
        "nltk.download('stopwords')  # For filtering common stopwords from the text\n",
        "\n",
        "# --------------------------\n",
        "# 2. Reddit API Setup\n",
        "# --------------------------\n",
        "# Reddit API credentials setup\n",
        "REDDIT_CLIENT_ID = \"h**\"\n",
        "REDDIT_CLIENT_SECRET = \"x**\"\n",
        "REDDIT_USER_AGENT = \"f**\"\n",
        "\n",
        "# Create a Reddit instance using PRAW (Python Reddit API Wrapper)\n",
        "reddit = praw.Reddit(\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_CLIENT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 3. Function Definitions\n",
        "# --------------------------\n",
        "\n",
        "# 3a. Function to fetch posts using Reddit search.\n",
        "@st.cache_data(show_spinner=False)\n",
        "def fetch_keyword_posts(keyword, subreddits=[\"fashion\"], time_filter=\"month\", limit=100):\n",
        "    \"\"\"\n",
        "    Searches for posts in each subreddit that mention 'keyword' using Reddit's search API.\n",
        "    'time_filter' allows you to search by time ranges such as \"day\", \"week\", \"month\", \"year\", or \"all\".\n",
        "    Returns a list of tuples: (created_utc, title, selftext).\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    for subreddit in subreddits:\n",
        "        # Searching in the specified subreddit for posts mentioning the keyword\n",
        "        for submission in reddit.subreddit(subreddit).search(keyword, sort=\"new\", time_filter=time_filter, limit=limit):\n",
        "            posts.append((submission.created_utc, submission.title, submission.selftext))\n",
        "    return posts\n",
        "\n",
        "# 3b. Function to fetch recent posts for overall trending keywords.\n",
        "@st.cache_data(show_spinner=False)\n",
        "def fetch_recent_posts(subreddits=[\"fashion\"], days=2, limit=200):\n",
        "    \"\"\"\n",
        "    Fetches posts from each subreddit using .new() and filters those created within the last 'days' days.\n",
        "    This helps identify trending topics by looking at the most recent posts.\n",
        "    Returns a list of tuples: (created_utc, title, selftext).\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    now = datetime.utcnow()\n",
        "    time_threshold = now - timedelta(days=days)  # Filters posts within the last 'days' days\n",
        "    for subreddit in subreddits:\n",
        "        for submission in reddit.subreddit(subreddit).new(limit=limit):\n",
        "            post_time = datetime.utcfromtimestamp(submission.created_utc)\n",
        "            if post_time > time_threshold:\n",
        "                posts.append((submission.created_utc, submission.title, submission.selftext))\n",
        "    return posts\n",
        "\n",
        "# 3c. Function to analyze timestamps of posts.\n",
        "@st.cache_data(show_spinner=False)\n",
        "def analyze_post_times(posts):\n",
        "    \"\"\"\n",
        "    Given a list of posts, counts how many were made in the last day, week, and month.\n",
        "    This helps in determining how recent or old a particular trend is.\n",
        "    Returns a dictionary with counts for \"day\", \"week\", and \"month\".\n",
        "    \"\"\"\n",
        "    now = datetime.utcnow()\n",
        "    counts = {\"day\": 0, \"week\": 0, \"month\": 0}\n",
        "    for created_utc, title, selftext in posts:\n",
        "        post_time = datetime.utcfromtimestamp(created_utc)\n",
        "        delta = now - post_time\n",
        "        if delta < timedelta(days=1):\n",
        "            counts[\"day\"] += 1\n",
        "        if delta < timedelta(weeks=1):\n",
        "            counts[\"week\"] += 1\n",
        "        if delta < timedelta(days=30):\n",
        "            counts[\"month\"] += 1\n",
        "    return counts\n",
        "\n",
        "# 3d. Function to extract keyword context snippets from text.\n",
        "def extract_keyword_snippets(text, keyword, window=5):\n",
        "    \"\"\"\n",
        "    Extracts snippets of context around the keyword occurrence in the text.\n",
        "    Returns snippets that include 'window' number of words before and after the keyword.\n",
        "    \"\"\"\n",
        "    pattern = r'(?:\\S+\\s+){0,' + str(window) + r'}\\b' + re.escape(keyword) + r'\\b(?:\\s+\\S+){0,' + str(window) + r'}'\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "    return matches\n",
        "\n",
        "# 3e. Function to get trend status for a given keyword.\n",
        "def get_trend_status(keyword, subreddits, time_filter=\"month\"):\n",
        "    \"\"\"\n",
        "    Analyzes how many posts mention the keyword over the last 'time_filter' period (day, week, month).\n",
        "    Uses thresholds to determine if a term is trending and extracts context snippets.\n",
        "    Returns a status message with trend details.\n",
        "    \"\"\"\n",
        "    posts = fetch_keyword_posts(keyword, subreddits=subreddits, time_filter=time_filter, limit=100)\n",
        "    if not posts:\n",
        "        return f\"The keyword '{keyword}' was not mentioned on Reddit in the selected time period.\"\n",
        "\n",
        "    counts = analyze_post_times(posts)\n",
        "    message = f\"For '{keyword}' over the last {time_filter}:\\n\"\n",
        "    message += f\"- Last day: {counts['day']} mentions\\n\"\n",
        "    message += f\"- Last week: {counts['week']} mentions\\n\"\n",
        "    message += f\"- Last month: {counts['month']} mentions\\n\\n\"\n",
        "\n",
        "    # Determine trend status based on mention counts\n",
        "    threshold_day = 5\n",
        "    threshold_week = 10\n",
        "    threshold_month = 15\n",
        "    if counts[\"day\"] > threshold_day or counts[\"week\"] > threshold_week:\n",
        "        message += \"This term appears to be trending!\\n\\n\"\n",
        "    else:\n",
        "        message += \"This term is not currently trending. Typically, a trending term gets at least \"\n",
        "        message += f\"{threshold_month} mentions in a month.\\n\\n\"\n",
        "\n",
        "    # Extract context snippets from posts.\n",
        "    snippet_list = []\n",
        "    total_occurrences = 0\n",
        "    for created_utc, title, selftext in posts:\n",
        "        title_snippets = extract_keyword_snippets(title, keyword, window=5)\n",
        "        body_snippets = extract_keyword_snippets(selftext, keyword, window=5)\n",
        "        occurrences = len(title_snippets) + len(body_snippets)\n",
        "        total_occurrences += occurrences\n",
        "        snippet_list.extend(title_snippets)\n",
        "        snippet_list.extend(body_snippets)\n",
        "\n",
        "    unique_snippets = list(dict.fromkeys(snippet_list))  # Remove duplicates\n",
        "    message += f\"Total keyword occurrences found: {total_occurrences}\\n\\n\"\n",
        "    if unique_snippets:\n",
        "        message += \"What are customers saying:\\n\"\n",
        "        for snippet in unique_snippets[:5]:  # Show up to 5 unique snippets\n",
        "            message += f\"\\\"{snippet}\\\"\\n\"\n",
        "    else:\n",
        "        message += \"No context excerpts could be extracted.\"\n",
        "    return message\n",
        "\n",
        "# 3f. Function to list overall trending keywords over the past 2 days.\n",
        "def find_trending_keywords(subreddits, days=2, min_occurrences=15):\n",
        "    \"\"\"\n",
        "    Fetches recent posts and extracts keywords mentioned more than 'min_occurrences' times.\n",
        "    Helps identify the most mentioned keywords across subreddits in the past 'days' days.\n",
        "    \"\"\"\n",
        "    posts = fetch_recent_posts(subreddits=subreddits, days=days, limit=200)\n",
        "    if not posts:\n",
        "        return \"No posts found in the selected time period.\"\n",
        "\n",
        "    combined_text = \" \".join(title + \" \" + selftext for (_, title, selftext) in posts)\n",
        "    tokens = word_tokenize(combined_text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in set(stopwords.words('english'))]\n",
        "    freq = Counter(filtered_tokens)\n",
        "    trending_words = [(word, count) for word, count in freq.items() if count >= min_occurrences]\n",
        "    trending_words.sort(key=lambda x: x[1], reverse=True)\n",
        "    if trending_words:\n",
        "        message = \"Trending keywords (mentioned at least {} times in the past {} days):\\n\".format(min_occurrences, days)\n",
        "        for word, count in trending_words:\n",
        "            message += f\"- {word}: {count} times\\n\"\n",
        "        return message\n",
        "    else:\n",
        "        return f\"No keywords were mentioned at least {min_occurrences} times in the past {days} days.\"\n",
        "\n",
        "# 3g. Set up an LLM for chat responses using GPT-2.\n",
        "llm = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "def generate_llm_response(prompt):\n",
        "    \"\"\"\n",
        "    Uses GPT-2 to generate responses for user inputs in the chatbot.\n",
        "    This allows for conversational interactions with the bot.\n",
        "    \"\"\"\n",
        "    response = llm(prompt, max_length=100, num_return_sequences=1)\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# --------------------------\n",
        "# 4. Streamlit App Interface\n",
        "# --------------------------\n",
        "st.title(\"What's Tredning on Fashion Reddit?\")\n",
        "st.write(\"\"\"\n",
        "This chatbot checks if a fashion-related keyword is trending on Reddit by analyzing recent mentions and provides context with excerpts.\n",
        "You can also ask: \"tell me what keyword is mentioned more than 15 times in fashion reddit for the past 2 days\"\n",
        "\"\"\")\n",
        "\n",
        "# Option to choose time filter for trend analysis (for keyword search)\n",
        "time_filter = st.selectbox(\"Select time period for trend analysis (search):\", [\"day\", \"week\", \"month\", \"year\", \"all\"])\n",
        "\n",
        "# Input for subreddits (comma-separated; default includes several fashion-related communities)\n",
        "subreddit_input = st.text_input(\"Enter subreddits to search (comma-separated):\", \"fashion, mensfashion, womensfashion, streetwear\")\n",
        "subreddits = [s.strip() for s in subreddit_input.split(\",\") if s.strip()]\n",
        "\n",
        "# User input for keyword (for direct trend analysis)\n",
        "keyword = st.text_input(\"Enter a fashion keyword for trend analysis:\")\n",
        "\n",
        "# Button to check trend for a given keyword (using search)\n",
        "if st.button(\"Check Trend for Keyword\"):\n",
        "    if not keyword.strip():\n",
        "        st.write(\"Please enter a keyword.\")\n",
        "    else:\n",
        "        with st.spinner(\"Fetching data...\"):\n",
        "            result = get_trend_status(keyword, subreddits, time_filter=time_filter)\n",
        "        st.write(result)\n",
        "\n",
        "# Button to list trending keywords (based on overall frequency) for the past 2 days.\n",
        "if st.button(\"List Trending Keywords (Past 2 Days)\"):\n",
        "    with st.spinner(\"Fetching recent posts and analyzing...\"):\n",
        "        trending_keywords_message = find_trending_keywords(subreddits, days=2, min_occurrences=15)\n",
        "    st.write(trending_keywords_message)\n",
        "\n",
        "# Chat Section for Additional Conversation\n",
        "st.write(\"### Chat with the Trend Bot\")\n",
        "user_input = st.text_input(\"Enter your message:\")\n",
        "if user_input:\n",
        "    st.write(\"**You:**\", user_input)\n",
        "    prompt = f\"User: {user_input}\\nBot:\"\n",
        "    llm_reply = generate_llm_response(prompt)\n",
        "    st.write(\"**Trend Bot:**\", llm_reply)"
      ],
      "metadata": {
        "id": "DnYWHbLhp_ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3232cb-0015-457c-b16c-8d32747f0961"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing pyngrok to create a secure tunnel to the local server for external access\n",
        "!pip install pyngrok\n",
        "\n",
        "# Importing the ngrok module from pyngrok to set up the tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token (replace with your own token from ngrok dashboard)\n",
        "# This is required to authenticate and access ngrok's features.\n",
        "ngrok.set_auth_token(\"2************************\")\n",
        "\n",
        "# Create a public URL that routes to the local streamlit app running on port 8501\n",
        "# This allows you to access the app from anywhere via the ngrok tunnel.\n",
        "public_url = ngrok.connect(addr=8501, proto=\"http\")\n",
        "\n",
        "# Print the public URL to the console so it can be accessed externally\n",
        "print(\"Streamlit URL:\", public_url)\n",
        "\n",
        "# Launch the Streamlit app in the background using the system's shell\n",
        "# This command runs the app without blocking the execution of the script.\n",
        "get_ipython().system_raw('streamlit run app.py &')\n"
      ],
      "metadata": {
        "id": "0MNarsBAqU7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d289cb-3268-4951-c686-87135ef061b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Streamlit URL: NgrokTunnel: \"https://fcca-34-56-90-159.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}
